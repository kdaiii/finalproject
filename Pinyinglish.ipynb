{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chinese, a language with a logographic writing system, differs tremendously from English, which uses a latin alphabet. As a young child growing up in China, I would often try to mimic the sounds of other languages using Mandarin phonemes with my friends. \n",
    "\n",
    "Although this phenomenon didn't have an official name at the time, it has slowly grown into a trend that is popular with Chinese netizens. It can be described loosely as \"中式外语\" (Chinese-Style Foreign Language), or \"汉子谐音\" (Chinese Character Homophones). Given the unofficial nature of this 'system', such techniques are discouraged by most Chinese, and is seen as an improper way to learn new languages, because it promotes inaccuracy when familiarizing oneself with the sounds of the target language. \n",
    "\n",
    "Nevertheless, it is still used online or as slang among the younger generation. To explore the relationship between Chinese characters and English, I wanted to create a program that 'translates' English sentences into Chinese characters purely based off of pronunciation, rather than meaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to utilize the google translate module in Python to aid in English-Chinese mapping, because the module offers a very unique function that I couldn't seem to find in other translate programs. More specifically, if one were to set both the destination and source language to Chinese and input pinyin into the source, the result would be the corresponding Chinese characters of the inputted pinyin. I'm not sure if this was intended by the original developers, but it saved me tons of time, because it meant that I essentially had a pinyin-to-Chinese translation device that did not require the presence of tonal marks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I felt that Regex was also going to play a big role in helping break down the source text into manageable, pinyin-like chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is really amazing\n"
     ]
    }
   ],
   "source": [
    "text_input = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the user input and check if it is a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Thi s i s  rea l ly a ma zi n g'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'(?i)([bcdfgjklmnpqrstvwxz])', r' \\1', text_input) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used regex to split the string at the English consonants, in order to create chunks that could be remotely recognizable as 'pinyin'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breakdown():\n",
    "    protoText = re.sub(r'(?i)([bcdfgjklmnpqrstvwxz])', r' \\1', text_input)\n",
    "    brokenText = protoText.lower()\n",
    "    for piece in [\"ch\",\"ee\",\"d\",\"oo\",\"th\",\"i\",\"v\"]:\n",
    "        if piece in brokenText:\n",
    "            brokenText = brokenText.replace(\"ch\",\"q\").replace(\"ee\",\"i\").replace(\" d \",\"de\").replace(\"oo\",\"u\").replace(\"th\",\"de\").replace(\"i\",\"ai\").replace(\"v\",\"w\")\n",
    "    print(brokenText)  \n",
    "    translations = translator.translate([brokenText], dest='zh-cn', src = 'zh-cn')\n",
    "    for translation in translations:\n",
    "        print(translation.origin, ' -> ', translation.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where I create my parameters from scratch. Since this is only a test, I was not too concerned with addressing the official sound-to-sound mapping of English and Chinese phonemes, but rather, I wanted to see if it was feasible for me to parse through the input text, replace English phonemes with Chinese phonemes, and feed it to the googletrans module for deciphering. The phoneme parameters are 'arbitrary' in that I mapped it according to my understanding of both languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dei s i s  rea l ly a ma zi n g\n",
      " dei s i s  rea l ly a ma zi n g  ->  迪亚斯·里·马津格\n"
     ]
    }
   ],
   "source": [
    "breakdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be honest, I was so surprised when the output appeared. The result was better than I expected, and it made me laugh, because it sounded exactly like me and my friends trying to imitate English when we were young. I was also very impressed with Google Translate, because it essentially had the ability to take English words (that are not fully pinyin), and then create a Chinese translation based off of little to no information. \n",
    "\n",
    "The input was \"This is really amazing\", and after running it through my parameters, became \"Dei s i s rea l ly a ma zi n g\", which translated into \"迪亚斯·里·马津格/di ya si, li, ma jin ge\". Given the structure of the output, I felt that the way I set my parameters was a good direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like noodles\n"
     ]
    }
   ],
   "source": [
    "text_input = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i  li ke  nu de le s\n",
      "i  li ke  nu de le s  ->  i立刻怒的垃圾\n"
     ]
    }
   ],
   "source": [
    "breakdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried the translation on another input source, to make sure it was versatile enough to accept a wide range of English sentences. As you can see, it did not work quite as well as the first input, because the 'i' did not get mapped to a Chinese character. However, because everything else in the sentence was quite accurate in terms of sound, I knew that it was the way I set my parameters that was the main issue. This wasn't a huge problem however, because I was planning to go back and map them according to official phoneme-to-phoneme rules.\n",
    "\n",
    "On a side note, this translation of \"I like noodles\" was also very funny. If a Chinese-speaker were to read the characters with no context, it would mean, \"Immediately angry trash\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every man is a piece of the continent\n"
     ]
    }
   ],
   "source": [
    "text_input = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e we ry  ma n aai s a  paaie ce o f  dai  co n taai ne n t\n",
      "e we ry  ma n aai s a  paaie ce o f  dai  co n taai ne n t  ->  e we ry ma n aai s a paaie ce o f dai co n taai ne n t\n"
     ]
    }
   ],
   "source": [
    "breakdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also inputted a verse from a poem with more complex words. The result was dissapointing; there was no instance of character-to-word mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eng_to_ipa as ipa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipa.convert(\"I like you because you are so cute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'aɪ laɪk ju bɪˈkəz ju ər soʊ kjut'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I imported an 'english to ipa' module to see if I could utilize the ipa system as a method for more precise Chinese-English mapping. After receiving the output, I realized that without a linguistics background, it would be quite difficult to make sense of all the different symbols, even with the IPA guide. Therefore, I decided to stick with pronouncing, as per my original plan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pronouncing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I like you because you are so cute\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = list()\n",
    "for word in text.split():\n",
    "    phones = pronouncing.phones_for_word(word)[0]\n",
    "    out.append(phones)\n",
    "print(' '.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AY1 L AY1 K Y UW1 B IH0 K AO1 Z Y UW1 AA1 R S OW1 K Y UW1 T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing pronouncing, I created a list \"out\", and then cycled through the text \"I like you because you are so cute\". Using the phones_for_word function in pronouncing, I acquired the pronounciations of each word, and stored them in variable phones. Then, I printed the result. This was much better than the IPA output because not only was it more intuitive (no special characters), I also had the ARPAbet guide, which essentially saved my life. Using the ARPAbet, I could now split the English words into chunks that follow a more 'scientific' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because \"out\" is still a list, I must convert it into a string so that it can be manipulated with Regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brokenText = str(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brokenText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['AY1', 'L AY1 K', 'Y UW1', 'B IH0 K AO1 Z', 'Y UW1', 'AA1 R', 'S OW1', 'K Y UW1 T']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(brokenText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the pieces all coming together, it was time to finish up the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like cheese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = list()\n",
    "for word in text.split():\n",
    "    phones = pronouncing.phones_for_word(word)[0]\n",
    "    out.append(phones)\n",
    "print(' '.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AY1 L AY1 K CH IY1 Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brokenText = ' '.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brokenText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AY1 L AY1 K CH IY1 Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I had the ARPAbet pronunciations, I noticed the numbers next to the outputs. These were the lexical stress markers, and I assumed they would be quite useful if I decided to include the tone markers of the Chinese language. However, given that the stressed and unstressed chunks resulted in essentially the same output on the Chinese side, I decided to just remove them and simplify the whole process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brokenText = re.sub('[012]', '', brokenText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brokenText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AY L AY K CH IY Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking much simpler. Now, it was time to map the ARPAbet pronunciations to their Chinese equivalent. I cycled through the entire ARPAbet documentation, and assigned each pronunciation with their closest Chinese phoneme. \n",
    "\n",
    "It should be noted that this whole process of \"中式外语\" (Chinese-Style Foreign Language) is not official, meaning that there is no scientifically approved, government backed system for 'correctly' mapping a Chinese a sound to the English equivalent (excluding IPA), at least, as far as I am aware of. Chinese netizens usually 'map' a word arbitrarily simply by deducing the closest sounding phonemes, usually with Chinese characters that sound the same, and have a meaning similar to the target word.\n",
    "\n",
    "Nevertheless, I acquired a list of Chinese sounds (I should also specify that I am using Standardized Mandarin pronunciation because it is my first language, and because I do not speak any other dialect), and then mapped them to the ARPAbet pronunciations using regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for piece in [\"AA\",\"AE\",\"AH\",\"AO\",\"AW\",\"AY\",\"B\",\"CH\",\"D\",\"DH\",\"EH\",\"ER\",\"EY\",\"F\",\"G\",\"HH\",\"IH\",\"IY\",\"JH\",\"K\",\"L\",\"M\",\"N\",\"NG\",\"OW\",\"OY\",\"P\",\"R\",\"S\",\"SH\",\"T\",\"TH\",\"UH\",\"UW\",\"V\",\"W\",\"Y\",\"Z\",\"ZH\"]:\n",
    "        if piece in brokenText:\n",
    "            brokenText = brokenText.replace(\"AA\",\"a\").replace(\"AE\",\"a\").replace(\"AH\",\"e\").replace(\"AO\",\"a\").replace(\"AW\",\"ao\").replace(\"AY\",\"ai\").replace(\"B\",\"bo\").replace(\"CH\",\"q\").replace(\"D\",\"de\").replace(\"DH\",\"de\").replace(\"EH\",\"ei\").replace(\"ER\",\"er\").replace(\"EY\",\"ei\").replace(\"F\",\"fu\").replace(\"G\",\"ge\").replace(\"HH\",\"he\").replace(\"IH\",\"yi\").replace(\"IY\",\"yi\").replace(\"JH\",\"ji\").replace(\"K\",\"ke\").replace(\"L\",\"le\").replace(\"M\",\"me\").replace(\"N\",\"ne\").replace(\"NG\",\"ge\").replace(\"OW\",\"ou\").replace(\"OY\",\"wo\").replace(\"P\",\"pa\").replace(\"R\",\"re\").replace(\"S\",\"si\").replace(\"SH\",\"she\").replace(\"T\",\"te\").replace(\"TH\",\"de\").replace(\"UH\",\"wu\").replace(\"UW\",\"yu\").replace(\"V\",\"fu\").replace(\"W\",\"wo\").replace(\"Y\",\"yi\").replace(\"Z\",\"zi\").replace(\"ZH\",\"zi\")\n",
    "print(brokenText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ai le ai ke q yi zi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output looks very clean, and with the exception of the 'q', everything else can be classified as 'pinyin' with no tone marks. Now, to check if it is a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(brokenText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the moment of truth: running this filtered output into the translator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = translator.translate([brokenText], dest='zh-cn', src = 'zh-cn')\n",
    "for translation in translations:\n",
    "    print(translation.origin, ' -> ', translation.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ai le ai ke q yi zi  ->  挨了挨可轻易自"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything worked smoothly and according to plan. The resulting Chinese characters were exactly what I had imagined in my mind before building the program, and reading it out loud next to the English input was very interesting. As a native Chinese speaker, I had to switch my mindset from focusing on the character meaning to the character sound instead. It was essentially, reading English in Chinese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinyinglish():\n",
    "    text = input()\n",
    "    \n",
    "    out = list()\n",
    "    for word in text.split():\n",
    "        phones = pronouncing.phones_for_word(word)[0]\n",
    "        out.append(phones)\n",
    "        \n",
    "    brokenText = ' '.join(out)\n",
    "    brokenText = re.sub('[012]', '', brokenText)\n",
    "    \n",
    "    for piece in [\"AA\",\"AE\",\"AH\",\"AO\",\"AW\",\"AY\",\"B\",\"CH\",\"DH\",\"D\",\"EH\",\"ER\",\"EY\",\"F\",\"G\",\"HH\",\"IH\",\"IY\",\"JH\",\"K\",\"L\",\"M\",\"N\",\"NG\",\"OW\",\"OY\",\"P\",\"R\",\"S\",\"SH\",\"T\",\"TH\",\"UH\",\"UW\",\"V\",\"W\",\"Y\",\"Z\",\"ZH\"]:\n",
    "        if piece in brokenText:\n",
    "            brokenText = brokenText.replace(\"AA\",\"a\").replace(\"AE\",\"a\").replace(\"AH\",\"e\").replace(\"AO\",\"a\").replace(\"AW\",\"ao\").replace(\"AY\",\"ai\").replace(\"B\",\"bo\").replace(\"CH\",\"q\").replace(\"DH\",\"de\").replace(\"D\",\"de\").replace(\"EH\",\"ei\").replace(\"ER\",\"er\").replace(\"EY\",\"ei\").replace(\"F\",\"fu\").replace(\"G\",\"ge\").replace(\"HH\",\"he\").replace(\"IH\",\"yi\").replace(\"IY\",\"yi\").replace(\"JH\",\"ji\").replace(\"K\",\"ke\").replace(\"L\",\"le\").replace(\"M\",\"me\").replace(\"N\",\"ne\").replace(\"NG\",\"ge\").replace(\"OW\",\"ou\").replace(\"OY\",\"wo\").replace(\"P\",\"pa\").replace(\"R\",\"re\").replace(\"S\",\"si\").replace(\"SH\",\"she\").replace(\"T\",\"te\").replace(\"TH\",\"si\").replace(\"UH\",\"wu\").replace(\"UW\",\"yu\").replace(\"V\",\"fu\").replace(\"W\",\"wo\").replace(\"Y\",\"yi\").replace(\"Z\",\"zi\").replace(\"ZH\",\"zi\")\n",
    "\n",
    "    translations = translator.translate([brokenText], dest='zh-cn', src = 'zh-cn')\n",
    "    for translation in translations:\n",
    "        print(translation.origin, ' -> ', translation.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinyinglish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like that picture of me\n",
    "\n",
    "ai le ai ke de a te pa yi ke q er e fu me yi  ->  挨了挨渴的阿特怕一颗球儿额驸么一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinyinglish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frozen snow above me\n",
    "\n",
    "fu re ou zi e ne si ne ou e bo e fu me yi  ->  覆热偶自饿呢死呢吽阿拨额驸么一"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I compiled everything into a handy function called Pinyinglish, and executed it to see the final result. The function worked on complex English sentences that were not able to be parsed previously. All outputs were very 'accurate' in mimicking English speech, and it was very interesting for me to be able to combine two very different languages together into one. \n",
    "\n",
    "The only thing left to do now was to convert the Chinese characters into actual pinyin with tone marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dragonmapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonmapper import hanzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinyinglish():\n",
    "    text = input()\n",
    "    \n",
    "    out = list()\n",
    "    for word in text.split():\n",
    "        phones = pronouncing.phones_for_word(word)[0]\n",
    "        out.append(phones)\n",
    "        \n",
    "    brokenText = ' '.join(out)\n",
    "    brokenText = re.sub('[012]', '', brokenText)\n",
    "    \n",
    "    for piece in [\"AA\",\"AE\",\"AH\",\"AO\",\"AW\",\"AY\",\"B\",\"CH\",\"DH\",\"D\",\"EH\",\"ER\",\"EY\",\"F\",\"G\",\"HH\",\"IH\",\"IY\",\"JH\",\"K\",\"L\",\"M\",\"N\",\"NG\",\"OW\",\"OY\",\"P\",\"R\",\"S\",\"SH\",\"T\",\"TH\",\"UH\",\"UW\",\"V\",\"W\",\"Y\",\"Z\",\"ZH\"]:\n",
    "        if piece in brokenText:\n",
    "            brokenText = brokenText.replace(\"AA\",\"a\").replace(\"AE\",\"a\").replace(\"AH\",\"e\").replace(\"AO\",\"a\").replace(\"AW\",\"ao\").replace(\"AY\",\"ai\").replace(\"B\",\"bo\").replace(\"CH\",\"q\").replace(\"DH\",\"de\").replace(\"D\",\"de\").replace(\"EH\",\"ei\").replace(\"ER\",\"er\").replace(\"EY\",\"ei\").replace(\"F\",\"fu\").replace(\"G\",\"ge\").replace(\"HH\",\"he\").replace(\"IH\",\"yi\").replace(\"IY\",\"yi\").replace(\"JH\",\"ji\").replace(\"K\",\"ke\").replace(\"L\",\"le\").replace(\"M\",\"me\").replace(\"N\",\"ne\").replace(\"NG\",\"ge\").replace(\"OW\",\"ou\").replace(\"OY\",\"wo\").replace(\"P\",\"pa\").replace(\"R\",\"re\").replace(\"S\",\"si\").replace(\"SH\",\"she\").replace(\"T\",\"te\").replace(\"TH\",\"si\").replace(\"UH\",\"wu\").replace(\"UW\",\"yu\").replace(\"V\",\"fu\").replace(\"W\",\"wo\").replace(\"Y\",\"yi\").replace(\"Z\",\"zi\").replace(\"ZH\",\"zi\")\n",
    "\n",
    "    translations = translator.translate([brokenText], dest='zh-cn', src = 'zh-cn')\n",
    "    for translation in translations:\n",
    "        print(\"\")\n",
    "        print(translation.text)\n",
    "        print(\"\")\n",
    "        print(dragonmapper.hanzi.to_pinyin(translation.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I imported the dragonmapper module, which is a Python module created specifically to work with Chinese characters and pinyin. The module has a really useful function \"hanzi.to_pinyin\", which converts hanzi (Chinese Characters) into their respective pinyin form. All I needed to do was to modify a bit of my Pinyinglish function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinyinglish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like cheese\n",
    "\n",
    "挨了挨可轻易自\n",
    "\n",
    "āile'āikěqīngyìzì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very last thing to do was to deploy the Pinyinglish program onto the web using Flask and Glitch. This step was pretty easy to do; I simply copied my code onto Glitch, and then imported Flask accordingly. To receive user input, I created a render_template html form, and then stored the user input into the 'text' variable as I did in Jupyter. \n",
    "\n",
    "I routed the 'app' to '/pinyinglish', then tested it in the browser. \n",
    "\n",
    "Here is the link to the Glitch hosted version: https://pinyinglish.glitch.me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some final thoughts on my Pinyinglish Project: I really enjoyed the entire process, and I learned a lot from start to finish. I've always wanted to explore the relationship between these two languages even before this course, so finishing this project was especially satisfying. \n",
    "\n",
    "I think that Pinyinglish also exhibits the fluidity and versatility of the Chinese writing system as well. I have a few friends that do not speak Chinese, and they always ask the question, \"isn't Chinese too complicated?\", or \"isn't writing Chinese impractical\", and most often, \"why don't you guys use an alphabet?\" Of course, hand-writing Chinese can be a bit cumbersome sometimes, especially if you're like me, and often forget the strokes after having written English for so long. \n",
    "\n",
    "However, the logographic nature of Chinese characters means that nearly everyone can adopt this writing system to suit their needs, because each character is focused on conveying the meaning, rather than the sound like alphabetic systems. In other words, people from different cultures (such as Japanese and Korean) are able to utilize Chinese characters in their languages, despite not being Chinese. This is also why China is able to become unified as one entity, despite the hundreds of dialects that are spoken across the country. \n",
    "\n",
    "I feel that Pinyinglish is able to bring out a different side to Chinese characters by scrapping the meaning, and emphasizing the sound instead. The result is quite interesting because it is a mix of two different writing systems into one.\n",
    "\n",
    "I hope that this project can push more people to appreciate the versatility of Chinese characters, and further explore the relationship between Chinese and languages like English. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
